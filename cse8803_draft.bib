@article{Che2015,
abstract = {We apply deep learning to the problem of discovery and detection of characteristic patterns of physiology in clinical time series data. We propose two novel modifications to standard neural net training that address challenges and ex-ploit properties that are peculiar, if not exclusive, to medical data. First, we examine a general framework for using prior knowledge to regularize parameters in the topmost layers. This framework can leverage priors of any form, ranging from formal ontologies (e.g., ICD9 codes) to data-derived similarity. Second, we describe a scalable procedure for training a collection of neural networks of di↵erent sizes but with partially shared architectures. Both of these innova-tions are well-suited to medical applications, where available data are not yet Internet scale and have many sparse out-puts (e.g., rare diagnoses) but which have exploitable struc-ture (e.g., temporal order and relationships between labels). However, both techniques are suciently general to be ap-plied to other problems and domains. We demonstrate the empirical ecacy of both techniques on two real-world hos-pital data sets and show that the resulting neural nets learn interpretable and clinically relevant features.},
author = {Che, Zhengping and Kale, David and Li, Wenzhe and {Taha Bahadori}, Mohammad and Liu, Yan},
doi = {10.1145/2783258.2783365},
file = {:home/hodapp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Che et al. - 2015 - Deep Computational Phenotyping.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Deep learning,Health,Healthcare,I51 [Pattern Recognition],ModelsNeural nets Keywords Medical informatics,Multi-label classification,Multivari-ate time series,Phenotyping,cse8803},
mendeley-tags = {cse8803},
pages = {507--516},
title = {{Deep Computational Phenotyping}},
url = {https://dl.acm.org/citation.cfm?id=2783365 http://www-scf.usc.edu/{~}zche/papers/kdd2015.pdf},
year = {2015}
}
@article{DeKeulenaer2009,
author = {{De Keulenaer}, Gilles W. and Brutsaert, Dirk L.},
doi = {10.1161/CIRCULATIONAHA.109.870006},
file = {:home/hodapp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Keulenaer, Brutsaert - 2009 - The heart failure spectrum Time for a phenotype-oriented approach.pdf:pdf},
issn = {00097322},
journal = {Circulation},
keywords = {cse8803},
mendeley-tags = {cse8803},
number = {24},
pages = {3044--3046},
pmid = {19506105},
title = {{The heart failure spectrum: Time for a phenotype-oriented approach}},
volume = {119},
year = {2009}
}
@article{Ghassemi2015,
abstract = {The ability to determine patient acuity (or severity of illness) has immediate practical use for clinicians. We evaluate the use of multivariate timeseries modeling with the multi-task Gaussian process (GP) models using noisy, incomplete, sparse, heterogeneous and unevenly- sampled clinical data, including both physiological sig- nals and clinical notes. The learned multi-task GP (MTGP) hyperparameters are then used to assess and forecast patient acuity. Experiments were conducted with two real clinical data sets acquired from ICU pa- tients: firstly, estimating cerebrovascular pressure reac- tivity, an important indicator of secondary damage for traumatic brain injury patients, by learning the inter- actions between intracranial pressure and mean arterial blood pressure signals, and secondly, mortality predic- tion using clinical progress notes. In both cases, MTGPs provided improved results: an MTGP model provided better results than single-task GP models for signal in- terpolation and forecasting (0.91 vs 0.69 RMSE), and the use of MTGP hyperparameters obtained improved results when used as additional classification features (0.812 vs 0.788 AUC). 1},
annote = {Note the use of MTGP over STGP. I guess MTGP allows correlation estimates where STGP would not?},
author = {Ghassemi, Marzyeh and Naumann, Tristan and Brennan, Thomas and Clifton, David a and Szolovits, Peter},
file = {:home/hodapp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghassemi et al. - 2015 - A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Spars.pdf:pdf},
isbn = {9781577356998},
issn = {2159-5399 (Print)},
journal = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
keywords = {Applications Track,cse8803},
mendeley-tags = {cse8803},
pages = {446--453},
pmid = {27182460},
title = {{A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Sparse , Heterogeneous Clinical Data}},
url = {http://mghassem.mit.edu/wp-content/uploads/2013/02/ghassemi{\_}AAAI2015{\_}multivariate{\_}timeseries{\_}modeling.pdf},
year = {2015}
}
@article{Johnson2016,
abstract = {Clinical data management systems typically provide caregiver teams with useful information, derived from large, sometimes highly heterogeneous, data sources that are often changing dynamically. Over the last decade there has been a significant surge in interest in using these data sources, from simply reusing the standard clinical databases for event prediction or decision support, to including dynamic and patient-specific information into clinical monitoring and prediction problems. However, in most cases, commercial clinical databases have been designed to document clinical activity for reporting, liability, and billing reasons, rather than for developing new algorithms. With increasing excitement surrounding “secondary use of medical records” and “Big Data” analytics, it is important to understand the limitations of current databases and what needs to change in order to enter an era of “precision medicine.” This review article covers many of the issues involved in the collection and preprocessing of critical care data. The three challenges in critical care are considered: compartmentalization, corruption, and complexity. A range of applications addressing these issues are covered, including the modernization of static acuity scoring; online patient tracking; personalized prediction and risk assessment; artifact detection; state estimation; and incorporation of multimodal data sources such as genomic and free text data.},
annote = {Good review paper that is fairly recent},
author = {Johnson, Alistair E W and Ghassemi, Mohammad M. and Nemati, Shamim and Niehaus, Katherine E. and Clifton, David and Clifford, Gari D.},
doi = {10.1109/JPROC.2015.2501978},
file = {:home/hodapp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2016 - Machine Learning and Decision Support in Critical Care.pdf:pdf},
isbn = {0018-9219 VO - 104},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Critical care,Feature extraction,Machine learning,Signal processing,cse8803,deep learning,healthcare,machine learning},
mendeley-tags = {cse8803,deep learning,healthcare,machine learning},
number = {2},
pages = {444--466},
title = {{Machine Learning and Decision Support in Critical Care}},
url = {http://ieeexplore.ieee.org/document/7390351/},
volume = {104},
year = {2016}
}
@article{Johnson2016a,
abstract = {MIMIC-III ('Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
author = {Johnson, Alistair E W and Pollard, Tom J and Shen, Lu and Lehman, Li-Wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark, Roger G},
doi = {10.1038/sdata.2016.35},
file = {:home/hodapp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care database.pdf:pdf},
issn = {2052-4463},
journal = {Scientific data},
keywords = {cse8803},
mendeley-tags = {cse8803},
pages = {160035},
pmid = {27219127},
title = {{MIMIC-III, a freely accessible critical care database.}},
url = {http://www.nature.com/articles/sdata201635{\%}5Cnpapers3://publication/doi/10.1038/sdata.2016.35},
volume = {3},
year = {2016}
}
@article{Klapper-Rybicka2001,
abstract = {hile much work has been done on unsupervised learning in feedforward neural network architectures, its potential with (theoretically more powerful) reecurrent networks and time-varying inputs has rarely been explored. Here we train Long Short-Term Memory (LSTM) recurrent networks to maximize two information-theoretic objectives for unsupervised learning: $\backslash$href{\{}http://nic.schraudolph.org/bib2html/b2hd-nips92.html{\}}{\{} Binary Information Gain Optimization{\}} and $\backslash$href{\{}http://nic.schraudolph.org/bib2html/b2hd-emma{\}}{\{} Nonparametric Entropy Optimization{\}}. LSTM learns to discriminate different types of temporal sequences and group them according to a variety of features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Klapper-Rybicka, Magdalena and Schraudolph, Nicol N. and Schmidhuber, J{\"{u}}rgen},
doi = {10.1007/3-540-44668-0},
eprint = {arXiv:1011.1669v3},
file = {:home/hodapp/source/bd4h-project/pdfs/Unsupervised Learning in LSTM Recurrent Neural Networks.pdf:pdf},
isbn = {978-3-540-42486-4},
issn = {16113349},
journal = {Icann},
keywords = {cse8803,recurrent neural networks,unsupervised learning},
mendeley-tags = {cse8803,recurrent neural networks,unsupervised learning},
pages = {674----681},
pmid = {25246403},
title = {{Unsupervised Learning in LSTM Recurrent Neural Networks}},
year = {2001}
}
@article{Lasko2015,
abstract = {Sampling repeated clinical laboratory tests with appropriate timing is challenging because the latent physiologic function being sampled is in general nonstationary. When ordering repeated tests, clinicians adopt various simple strategies that may or may not be well suited to the behavior of the function. Previous research on this topic has been primarily focused on cost-driven assessments of oversampling. But for monitoring physiologic state or for retrospective analysis, undersampling can be much more problematic than oversampling. In this paper we analyze hundreds of observation sequences of four different clinical laboratory tests to provide principled, data-driven estimates of undersampling and oversampling, and to assess whether the sampling adapts to changing volatility of the latent function. To do this, we developed a new method for fitting a Gaussian process to samples of a nonstationary latent function. Our method includes an explicit estimate of the latent function's volatility over time, which is deterministically related to its nonstationarity. We find on average that the degree of undersampling is up to an order of magnitude greater than oversampling, and that only a small minority are sampled with an adaptive strategy.},
annote = {Recommended to me by Lasko himself: 
"BTW, that was meant as a quick-and-dirty approach to time warping. It works only in the case where you expect the sampling frequency to be proportional to the volatility of the thing being measured. This is true (enough) for uric acid. One would hope it is true for all medical tests, but it turns out that it isn't. For a more robust approach that doesn't make this assumption, see:"},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Lasko, Thomas A},
doi = {10.1161/CIRCRESAHA.116.303790.The},
eprint = {15334406},
file = {:home/hodapp/source/bd4h-project/pdfs/Nonstationary Gaussian Process Regression etc.pdf:pdf},
isbn = {0324141122},
issn = {2159-5399},
journal = {Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence},
keywords = {autophagy,mitophagy,parkin,pink1},
month = {jan},
number = {8},
pages = {1777--1783},
pmid = {26097785},
title = {{Nonstationary Gaussian Process Regression for Evaluating Clinical Laboratory Test Sampling Strategies.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26097785 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4472307},
volume = {2015},
year = {2015}
}
@article{Lasko2013,
abstract = {Inferring precise phenotypic patterns from population-scale clinical data is a core computational task in the development of precision, personalized medicine. The traditional approach uses supervised learning, in which an expert designates which patterns to look for (by specifying the learning task and the class labels), and where to look for them (by specifying the input variables). While appropriate for individual tasks, this approach scales poorly and misses the patterns that we don't think to look for. Unsupervised feature learning overcomes these limitations by identifying patterns (or features) that collectively form a compact and expressive representation of the source data, with no need for expert input or labeled examples. Its rising popularity is driven by new deep learning methods, which have produced high-profile successes on difficult standardized problems of object recognition in images. Here we introduce its use for phenotype discovery in clinical data. This use is challenging because the largest source of clinical data – Electronic Medical Records – typically contains noisy, sparse, and irregularly timed observations, rendering them poor substrates for deep learning methods. Our approach couples dirty clinical data to deep learning architecture via longitudinal probability densities inferred using Gaussian process regression. From episodic, longitudinal sequences of serum uric acid measurements in 4368 individuals we produced continuous phenotypic features that suggest multiple population subtypes, and that accurately distinguished (0.97 AUC) the uric-acid signatures of gout vs. acute leukemia despite not being optimized for the task. The unsupervised features were as accurate as gold-standard features engineered by an expert with complete knowledge of the domain, the classification task, and the class labels. Our findings demonstrate the potential for achieving computational phenotype discovery at population scale. We expect such data-driven phenotypes to expose unknown disease variants and subtypes and to provide rich targets for genetic association studies. Citation: Lasko TA, Denny JC, Levy MA (2013) Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clinical Data. PLoS ONE 8(6): e66341.},
annote = {Recommended by Dr. Jimeng Sun. "I know the author and this paper is well cited. We are working on related area on deep learning but not yet dealing with the lab results yet."},
author = {Lasko, Thomas A. and Denny, Joshua C. and Levy, Mia A.},
doi = {10.1371/journal.pone.0066341},
file = {:home/hodapp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lasko, Denny, Levy - 2013 - Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clin.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
keywords = {cse8803,deep learning,gaussian processes,healthcare,machine learning,unsupervised learning},
mendeley-tags = {cse8803,deep learning,gaussian processes,healthcare,machine learning,unsupervised learning},
number = {6},
pmid = {23826094},
title = {{Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clinical Data}},
url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0066341},
volume = {8},
year = {2013}
}
@article{Lipton2015,
abstract = {Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines on a wide variety of metrics, and nearly match the performance of a multilayer perceptron trained on hand-engineered features, establishing the usefulness of LSTMs for modeling medical data. The best LSTM model accurately classifies many diagnoses, including diabetic ketoacidosis (F1 score of .933), status asthmaticus (.653), and scoliosis (.582).},
annote = {Very similar to "Directly Modeling Missing Data etc." from Lipton et al. I think},
archivePrefix = {arXiv},
arxivId = {1511.03677},
author = {Lipton, Zachary C. and Kale, David C. and Elkan, Charles and Wetzell, Randall},
eprint = {1511.03677},
file = {:home/hodapp/source/bd4h-project/pdfs/Learning to Diagnose with LSTM Recurrent Neural Networks.pdf:pdf},
journal = {Iclr},
keywords = {cse8803,healthcare,recurrent neural networks},
mendeley-tags = {cse8803,healthcare,recurrent neural networks},
pages = {1--18},
title = {{Learning to Diagnose with LSTM Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.03677},
year = {2015}
}
@article{Lipton2016,
abstract = {We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the intensive care unit (ICU) of a major urban medical center, our data consists of multivariate time series of observations. The data is irregularly sampled, leading to missingness patterns in re-sampled sequences. In this work, we show the remarkable ability of RNNs to make effective use of binary indicators to directly model missing data, improving AUC and F1 significantly. However, while RNNs can learn arbitrary functions of the missing data and observations, linear models can only learn substitution values. For linear models and MLPs, we show an alternative strategy to capture this signal. Additionally, we evaluate LSTMs, MLPs, and linear models trained on missingness patterns only, showing that for several diseases, what tests are run can be more predictive than the results themselves.},
annote = {This seems tailored to the supervised learning case},
archivePrefix = {arXiv},
arxivId = {1606.04130},
author = {Lipton, Zachary C. and Kale, David C. and Wetzel, Randall},
eprint = {1606.04130},
file = {:home/hodapp/source/bd4h-project/pdfs/Modeling Missing Data in Clinical Time Series with RNNs.pdf:pdf},
keywords = {cse8803,recurrent neural networks},
mendeley-tags = {cse8803,recurrent neural networks},
number = {2016},
pages = {1--17},
title = {{Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series}},
url = {http://arxiv.org/abs/1606.04130},
volume = {56},
year = {2016}
}
@article{Lipton2015a,
abstract = {We present a novel application of LSTM recurrent neural networks to multilabel classification of diagnoses given variable-length time series of clinical measurements. Our method outperforms a strong baseline on a variety of metrics.},
archivePrefix = {arXiv},
arxivId = {1510.07641},
author = {Lipton, Zachary C. and Kale, David C. and Wetzell, Randall C.},
eprint = {1510.07641},
file = {:home/hodapp/source/bd4h-project/pdfs/Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {cse8803,electronic health records,recurrent neural networks,regularization,temporal graph,temporal phenotyping},
mendeley-tags = {cse8803,recurrent neural networks},
pages = {705--714},
title = {{Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1510.07641},
year = {2015}
}
@article{Marlin,
abstract = {Bedside clinicians routinely identify temporal patterns in physiologic data in the process of choosing and administering treatments intended to alter the course of critical illness for individual patients. Our primary interest is the study of unsupervised learning techniques for automatically uncovering such patterns from the physiologic time series data contained in electronic health care records. This data is sparse, highdimensional and often both uncertain and incomplete. In this paper, we develop and study a probabilistic clustering model designed to mitigate the effects of temporal sparsity inherent in electronic health care records data. We evaluate the model qualitatively by visualizing the learned cluster parameters and quantitatively in terms of its ability to predict mortality outcomes associated with patient episodes. Our results indicate that the model can discover distinct, recognizable physiologic patterns with prognostic significance.},
author = {Marlin, Benjamin M and Kale, David C and Khemani, Robinder G and Wetzel, Randall C},
doi = {10.1145/2110363.2110408},
file = {:home/hodapp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marlin et al. - Unknown - Unsupervised Pattern Discovery in Electronic Health Care Data Using Probabilistic Clustering Models.pdf:pdf},
keywords = {cse8803},
mendeley-tags = {cse8803},
title = {{Unsupervised Pattern Discovery in Electronic Health Care Data Using Probabilistic Clustering Models}}
}
@article{Prasad2014,
abstract = {Ability of deep networks to extract high level features and of recurrent networks to perform time-series inference have been studied. In view of universality of one hidden layer network at approximating functions under weak constraints, the benefit of multiple layers is to enlarge the space of dynamical systems approximated or, given the space, reduce the number of units required for a certain error. Traditionally shallow networks with manually engineered features are used, back-propagation extent is limited to one and attempt to choose a large number of hidden units to satisfy the Markov condition is made. In case of Markov models, it has been shown that many systems need to be modeled as higher order. In the present work, we present deep recurrent networks with longer backpropagation through time extent as a solution to modeling systems that are high order and to predicting ahead. We study epileptic seizure suppression electro-stimulator. Extraction of manually engineered complex features and prediction employing them has not allowed small low-power implementations as, to avoid possibility of surgery, extraction of any features that may be required has to be included. In this solution, a recurrent neural network performs both feature extraction and prediction. We prove analytically that adding hidden layers or increasing backpropagation extent increases the rate of decrease of approximation error. A Dynamic Programming (DP) training procedure employing matrix operations is derived. DP and use of matrix operations makes the procedure efficient particularly when using data-parallel computing. The simulation studies show the geometry of the parameter space, that the network learns the temporal structure, that parameters converge while model output displays same dynamic behavior as the system and greater than .99 Average Detection Rate on all real seizure data tried.},
annote = {Very math-heavy. I mostly skimmed.},
archivePrefix = {arXiv},
arxivId = {1407.5949},
author = {Prasad, Sharat C and Prasad, Piyush},
eprint = {1407.5949},
file = {:home/hodapp/source/bd4h-project/pdfs/Deep Recurrent Neural Networks for Time-Series Prediction.pdf:pdf},
keywords = {cse8803,recurrent neural networks},
mendeley-tags = {cse8803,recurrent neural networks},
pages = {1--54},
title = {{Deep Recurrent Neural Networks for Time Series Prediction}},
url = {http://arxiv.org/abs/1407.5949},
volume = {95070},
year = {2014}
}
@book{Rasmussen2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Rasmussen, Carl E. and Williams, Christopher K. I.},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
file = {:home/hodapp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen, Williams - 2004 - Gaussian processes for machine learning.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
keywords = {2006,c,c 2006 massachusetts institute,cse8803,e,gaussian processes for machine,gaussianprocess,gpml,i,isbn 026218253x,k,learning,of technology,org,rasmussen,the mit press,williams,www},
mendeley-tags = {cse8803},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
volume = {14},
year = {2004}
}
@article{Rolfe2013,
abstract = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
annote = {DrSAE = Discriminative Recurrent Sparse Auto-Encoder

This is the only paper I've found so far that covers the use of RNNs like autoencoders. However, what does 'discriminative' mean in this context?},
archivePrefix = {arXiv},
arxivId = {1301.3775},
author = {Rolfe, Jason Tyler and LeCun, Yan},
eprint = {1301.3775},
file = {:home/hodapp/source/bd4h-project/pdfs/Discriminative Recurrent Sparse Auto-Encoders.pdf:pdf},
journal = {CoRR},
keywords = {cse8803,unsupervised learning},
mendeley-tags = {cse8803,unsupervised learning},
pages = {15},
title = {{Discriminative Recurrent Sparse Auto-Encoders}},
url = {http://arxiv.org/abs/1301.3775},
year = {2013}
}
@article{Strub2015,
annote = {This doesn't mention much on time-series but perhaps it will work regardless},
author = {Strub, Florian and Jeremie, Mary},
file = {:home/hodapp/source/bd4h-project/pdfs/Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs.pdf:pdf},
journal = {NIPS Workshop on Machine Learning for eCommerce},
keywords = {autoencoders,cse8803,neural networks,sparse inputs},
mendeley-tags = {autoencoders,cse8803,neural networks,sparse inputs},
title = {{Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs}},
url = {https://hal.archives-ouvertes.fr/hal-01256422/document},
year = {2015}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:home/hodapp/source/bd4h-project/pdfs/Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Advances in Neural Information Processing Systems (NIPS)},
keywords = {cse8803,lstm,recurrent neural networks},
mendeley-tags = {cse8803,lstm,recurrent neural networks},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{Werbos1990,
abstract = {Basic backpropagation, which is a simple method now being widely$\backslash$nused in areas like pattern recognition and fault diagnosis, is reviewed.$\backslash$nThe basic equations for backpropagation through time, and applications$\backslash$nto areas like pattern recognition involving dynamic systems, systems$\backslash$nidentification, and control are discussed. Further extensions of this$\backslash$nmethod, to deal with systems other than neural networks, systems$\backslash$ninvolving simultaneous equations, or true recurrent networks, and other$\backslash$npractical issues arising with the method are described. Pseudocode is$\backslash$nprovided to clarify the algorithms. The chain rule for ordered$\backslash$nderivatives-the theorem which underlies backpropagation-is briefly$\backslash$ndiscussed. The focus is on designing a simpler version of$\backslash$nbackpropagation which can be translated into computer code and applied$\backslash$ndirectly by neutral network users},
annote = {Linked from https://deeplearning4j.org/lstm.html},
author = {Werbos, Paul J.},
doi = {10.1109/5.58337},
file = {:home/hodapp/source/bd4h-project/pdfs/Backprop Through TIme.pdf:pdf},
isbn = {0018-9219},
issn = {15582256},
journal = {Proceedings of the IEEE},
keywords = {cse8803,neural networks,recurrent neural networks},
mendeley-tags = {cse8803,neural networks,recurrent neural networks},
number = {10},
pages = {1550--1560},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
volume = {78},
year = {1990}
}
